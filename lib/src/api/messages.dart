part of kafka;

/// Kafka Message Attributes. Only [KafkaCompression] is supported by the
/// server at the moment.
class MessageAttributes {
  /// Compression codec.
  final KafkaCompression compression;

  /// Creates new instance of MessageAttributes.
  MessageAttributes([this.compression = KafkaCompression.none]);

  /// Creates MessageAttributes from the raw byte.
  MessageAttributes.readFrom(int byte) : compression = getCompression(byte);

  static KafkaCompression getCompression(int byte) {
    var c = byte & 3;
    switch (c) {
      case 0:
        return KafkaCompression.none;
      case 1:
        return KafkaCompression.gzip;
      case 2:
        return KafkaCompression.snappy;
      default:
        throw new StateError('Unsupported compression codec: ${c}.');
    }
  }

  /// Converts this attributes into byte.
  int toInt() {
    return _compressionToInt();
  }

  int _compressionToInt() {
    switch (this.compression) {
      case KafkaCompression.none:
        return 0;
      case KafkaCompression.gzip:
        return 1;
      case KafkaCompression.snappy:
        return 2;
    }
  }
}

/// Kafka Message as defined in the protocol.
class Message {
  final int magicByte;
  final MessageAttributes attributes;
  final List<int> value;
  final List<int> key;

  /// Default constructor.
  Message(this.magicByte, this.attributes, this.key, this.value);

  /// Creates new Message from string value.
  static Message fromString(String value,
      [MessageAttributes attributes, List<int> key]) {
    var valueInBytes = UTF8.encode(value);
    attributes ??= new MessageAttributes();
    return new Message(0, attributes, key, valueInBytes);
  }

  /// Creates new instance of Message from the received data.
  static Message readFrom(KafkaBytesReader reader) {
    var magicByte = reader.readInt8();
    var attributes = new MessageAttributes.readFrom(reader.readInt8());
    var key = reader.readBytes();
    var value = reader.readBytes();
    return new Message(magicByte, attributes, key, value);
  }

  /// Converts Message to byte array.
  List<int> toBytes() {
    var builder = new KafkaBytesBuilder();
    builder.addInt8(magicByte);
    builder.addInt8(attributes.toInt());
    builder.addBytes(key);
    builder.addBytes(value);

    var data = builder.takeBytes();
    int crc = Crc32.signed(data);
    builder.addInt32(crc);
    builder.addRaw(data);

    return builder.toBytes();
  }

  /// Returns value of this message converted into a string.
  String asString() {
    return UTF8.decode(this.value);
  }
}

/// Kafka MessageSet type as defined in the protocol specification.
class MessageSet {
  /// Collection of messages. Keys in the map are message offsets.
  final Map<int, Message> _messages = new Map();

  Map get messages => new UnmodifiableMapView(_messages);

  /// Number of messages in this message set.
  int get length => _messages.length;

  /// Creates new empty message set.
  MessageSet();

  /// Creates new MessageSet from provided data.
  MessageSet.readFrom(KafkaBytesReader reader, [Logger logger]) {
    int messageSize = 0;
    while (reader.eof == false) {
      try {
        int offset = reader.readInt64();
        messageSize = reader.readInt32();
        var crc = reader.readInt32();

        var data = reader.readRaw(messageSize - 4);
        var actualCrc = Crc32.signed(data);
        if (actualCrc != crc) {
          // TODO: Maybe catch this error below and just break out of this loop?
          throw new MessageCrcMismatchError(
              'Expected crc: ${crc}, actual: ${actualCrc}');
        }
        var messageReader = new KafkaBytesReader.fromBytes(data);
        var message = Message.readFrom(messageReader);
        this._messages[offset] = message;
      } on RangeError {
        // According to spec server is allowed to return partial
        // messages, so we just ignore it here and exit the loop.
        var remaining = reader.length - reader.offset;
        logger?.info(
            'Encountered partial message. Expected message size: ${messageSize}, bytes left in buffer: ${remaining}');
        break;
      }
    }
  }

  /// Adds [Message] to this message set.
  ///
  /// Offset for this new message will be autogenerated.
  void addMessage(Message message) {
    var offset = _messages.length;
    _messages[offset] = message;
  }

  /// Converts this MessageSet into sequence of bytes conforming to Kafka
  /// protocol spec.
  List<int> toBytes() {
    var builder = new KafkaBytesBuilder();
    _messages.forEach((offset, message) {
      var messageData = message.toBytes();
      builder.addInt64(offset);
      builder.addInt32(messageData.length);
      builder.addRaw(messageData);
    });

    return builder.toBytes();
  }
}
